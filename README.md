# SDU-AI-Security
计算机安全知识点梳理
## 对抗样本
* 出现攻击与消失攻击
* 距离函数
* 定向攻击与非定向攻击
* 训练与攻击其实是对偶关系，训练是想让学习一个分类器，使得输入的预测结果与真实标签尽量接近，而攻击是找到一个扰动使损失函数最大化，从而让模型预测错误
* 梯度下降的思路，找到一个小扰动 𝛿让损失函数最大化，结合泰勒展开等等。
* 线性模型的对抗样本，在高维空间中，每一维的扰动积累起来，可能导致整体输出的大幅度变化，也会收到攻击
* DNN大多是分段线性的，比如卷积层，分段线性函数ReLU,Sigmoid函数不是线性的，但在一定范围内近似为线性。神经网络的线性特性导致了对抗样本的脆弱性
* 对抗样本的对抗扰动不是随机的，是需要计算梯度最大处刻意安排
* 不同对抗攻击方法原理类似，差异会因为`损失函数`,`约束条件`,`优化算法`而不同。
* 投影梯度下降法，Carlini-Wagner攻击，单像素攻击，Universal Adversarial Perturbation通用对抗扰动，Zeroth Order Optimization(ZOO)零阶优化方法
* Kerckhoffs原则:即便攻击者了解系统的设计和算法，只要密钥安全，系统就应该安全。
* 防御策略:1数据预处理，输入模型前移除对抗噪声。2模型加固，修改模型架构，训练流程以提升鲁棒性。(对抗训练，正则化，网络结构改进)3对抗样本检测，(输入特征分析，置信度阈值，特定网络检测器
* 防御性蒸馏
* 对抗训练，在训练过程中引入对抗扰动，强制模型在有攻击的情况下也能正确分类。限制:损失函数不是连续可微的，因为神经网络中的 ReLU 和 max-pooling 操作会引入非光滑性,我们只能近似找到最优扰动，而非严格意义上的全局最优.慢且不具可扩展性, 训练速度慢 2~20 倍,如果扰动弱，模型可能并未真正学到鲁棒性。
## 模型后门与数据投毒
### 数据投毒事后门攻击的主要手段
* Outsourcing Attack（外包攻击）,中间人搞事
* Pretrained Attack（预训练模型攻击），攻击者可以下载一个流行的预训练模型，在其中植入后门再重新发布给公众使用。开源社区中流行的预训练模型极易成为攻击的载体。思路:诱导模型增强局部链接，1.寻找最大化激活某个神经元的 pattern 2.逆向生成最大化某类别的训练数据 3.逆向数据 + pattern => 重新训练模型
* Data Collection Attack （数据收集攻击）受害者从公共来源收集数据，却没有意识到其中一些数据已经被投毒
* Targeted Clean-Label Attack 有针对性的干净标签攻击，攻击者不能直接篡改标签，只能投毒图像，感觉像对抗样本
* Watermarking(水印攻击),Multiple Instance Attack(多实例攻击),Collaborative Learning Attack(联邦学习攻击),Post-Deployment Attack（部署后攻击）方式可能是:攻击者入侵系统或服务器后，获得模型访问权限，直接在模型本身上动手脚，权重篡改攻击：修改模型权重，实现后门效果。比特翻转攻击：翻转存储模型的内存中的比特位，从而改变模型行为，Code Poisoning Attack(代码投毒攻击）代码中嵌入了后门逻辑，旨在污染训练后的模型
* 用于善意目的的后门攻击,水印，知识产权保护
### 防御手段
* ABS（Activation-based Backdoor Scanning, 基于激活的后门检测）方法。后门神经元在被特定触发器激活时，会导致目标输出类别异常激活。这种现象体现在某些神经元的激活值达到特殊水平时，网络输出几乎只依赖这些神经元，而对其他神经元不敏感。正常模型不会出现“单个神经元控制输出”的异常模式。
## 模型提取
方程求解方法与通过学习模拟的方法
攻击者（adversarial client）通过尽可能少的查询，学习到目标模型 𝑓的“近似拷贝” 𝑓′,使得 𝑓′(𝑥) 和f(x) 在 99.9% 以上输入上输出一致。
* 对逻辑回归模型的提取攻击，变换方程根据输入输出推测w和b
* Generic Equation-Solving Attack（通用方程求解攻击）攻击者准备一批输入 X,这些输入送入远程的机器学习服务,服务返回置信度,用优化算法（如梯度下降）反复调整 W，使损失最小。总结来说构造输入 → 查询模型 → 收集输出 → 拟合自己的神经网络参数
* 模型反演攻击（Model Inversion Attack），利用模型提取的结果，推出训练数据
* Extracting a Decision Tree输入𝑥和x′仅有一个特征不同。如果x和x′最终落在不同的叶节点，说明树在这个特征上进行了分裂。通过构造和查询不同输入，可以一步步推断出决策树在各特征上的分裂点，最终还原整棵树
* Generic Model Retraining Attack（通用模型重训练攻击）远不如方程求解法高效。主动学习可以大幅提升模型“扒取”效率，但在深度非线性模型下仍比直接方程求解低效许多
### 防御
* API最小化是一种防御策略,只返回类别标签，不返回置信度分数。但是，如果攻击者能用成员资格查询，依然能学习模型（如低效攻击）
* 限制预测信息，例如仅返回类别标签，而不返回或修改/隐藏/四舍五入置信度值
* 使用多个模型的组合来增加攻击难度，提高鲁棒性
* 通过差分隐私技术保护模型参数，防止攻击者通过查询推测模型内部信息。
### 学习模拟的思路
* Knockoff nets 拿别人的输出自己学
* 半监督学习
* 借助密码分析方法进行分析:ReLU的二阶导为0 & 有限差分
*  Data-Free Model Extraction ......
 ## 数据提取 
 * 模型反演攻击（Model Inversion Attack），利用模型提取的结果，推出训练数据
 * Data Extraction Attack（数据提取攻击）
 * Data Stealing Attack（数据窃取攻击）
 * Training Data Extraction Attack（训练数据提取攻击）
 * Model Memorization Attack（模型记忆攻击）
 * Model Inversion Attack（模型反演攻击）
```
   模型容量 (Model Capacity)
定义：DNN的参数数量通常远大于训练样本数，使其具有记住训练数据的能力。

示例：ResNet-50有约2500万个参数，而CIFAR-10数据集仅包含6万张图像。

优化目标 (Optimization Objective)
定义：DNN的训练目标是最小化损失函数（如交叉熵），可能导致对噪声或异常样本的过拟合。

示例：若某张图像被错误标注，DNN可能通过记住该样本来降低损失。

数据分布 (Data Distribution)
定义：训练数据中的噪声、重复样本或敏感信息可能被DNN记住。

示例：医疗数据中的罕见病例可能被DNN记住，导致隐私泄露。
```
### 方法
* 梯度下降
```
函数 MI-FACE(label, α, β, γ, λ):
    c(x) 定义为 1 - f_label(x) + AUXTERM(x)  # 损失函数定义
    x₀ ← 0                                   # 初始化输入

    对 i = 1 到 α:
        xᵢ ← PROCESS(xᵢ₋₁ - λ ⋅ ∇c(xᵢ₋₁))    # 通过梯度下降更新输入

        如果 c(xᵢ) ≥ max(c(xᵢ₋₁), ..., c(xᵢ₋β)) :
            break                             # 若损失大于过去β次中的最大值，则终止

        如果 c(xᵢ) ≤ γ :
            break                             # 如果损失小于阈值γ，则终止

    返回 [argminₓ c(xᵢ), minₓ c(xᵢ)]         # 返回损失最小的输入和对应的损失值
```
### 防御
* 四舍五入处理，通过减少置信度值的精度，来削弱攻击者从模型输出中获取过多信息的能力
* Exposure-based Testing Method ，通过加入随机的训练数据(金丝雀canary)，看看模型查询模型，看其是否能够回忆出。若能回忆出，说明模型具有较强的记忆能力，存在泄漏训练数据的风险

总结

```
无意记忆的发生并不依赖于过拟合的产生，甚至都不是过度训练导致的
不常见的随机训练数据会在模型达到最大效用之前就被记住了
weight decay, dropout, and quantization 三种 regularization 方法均不能解决记忆问题
使用 Differential Privacy 可以防止模型记忆化，但模型能力也被削弱
```
## 隐私推理
*  Membership Inference Attack (MIA)判断一个用户是否参与了模型的训练数据。最常见的思路从数据集 𝐷中抽样出多个子集,在每个子集上训练一个模型,选择其中一个模型作为目标模型, 其余的模型作为影子模型
*  MIA局限性:需要构造影子模型（shadow models），增加攻击复杂度和成本.假设攻击者能够获取某些数据或具有先验知识。模型必须发生过拟合，攻击才能有效。无过拟合时，成员和非成员样本表现趋同，难以区分.当前大多数方法仅适用于分类模型。攻击在大模型上效果下降，主要在小模型（如简单CNN）上实验验证
*  基于指标的异常检测:预测正确性(预测正确的就是成员),预测损失(低于训练样本平均损失的是成员),预测置信度(有概率接近1的是成员),预测熵(低概率熵的是成员),修正预测熵
*  Property Inference属性推断.举例：即使性别不是数据集的特征之一，也可以从训练模型的数据集中推断出女性和男性患者的比例
*  Attribute Inference特征值推断
### 防范措施
对数据隐私的保护
* 推理阶段的数据隐私保护算法：在模型的推理阶段施加防御手段；
* 训练阶段的数据隐私保护算法：在模型的训练阶段施加防御手段；
对模型版权的保护
* 数字水印（Digital Watermark）
* 用 AE 的思想抵抗成员推理模型的分类结果。
