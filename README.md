# SDU-AI-Security
计算机安全知识点梳理
## 对抗样本
* 出现攻击与消失攻击
* 距离函数
* 定向攻击与非定向攻击
* 训练与攻击其实是对偶关系，训练是想让学习一个分类器，使得输入的预测结果与真实标签尽量接近，而攻击是找到一个扰动使损失函数最大化，从而让模型预测错误
* 梯度下降的思路，找到一个小扰动 𝛿让损失函数最大化，结合泰勒展开等等。
* 线性模型的对抗样本，在高维空间中，每一维的扰动积累起来，可能导致整体输出的大幅度变化，也会收到攻击
* DNN大多是分段线性的，比如卷积层，分段线性函数ReLU,Sigmoid函数不是线性的，但在一定范围内近似为线性。神经网络的线性特性导致了对抗样本的脆弱性
* 对抗样本的对抗扰动不是随机的，是需要计算梯度最大处刻意安排
* 不同对抗攻击方法原理类似，差异会因为`损失函数`,`约束条件`,`优化算法`而不同。
* 投影梯度下降法，Carlini-Wagner攻击，单像素攻击，Universal Adversarial Perturbation通用对抗扰动，Zeroth Order Optimization(ZOO)零阶优化方法
* Kerckhoffs原则:即便攻击者了解系统的设计和算法，只要密钥安全，系统就应该安全。
* 防御策略:1据预处理，输入模型前移除对抗噪声。2模型加固，修改模型架构，训练流程以提升鲁棒性。(对抗训练，正则化，网络结构改进)3对抗样本检测，(输入特征分析，置信度阈值，特定网络检测器
* 防御性蒸馏
* 对抗训练，在训练过程中引入对抗扰动，强制模型在有攻击的情况下也能正确分类。限制:损失函数不是连续可微的，因为神经网络中的 ReLU 和 max-pooling 操作会引入非光滑性,我们只能近似找到最优扰动，而非严格意义上的全局最优.慢且不具可扩展性, 训练速度慢 2~20 倍,如果扰动弱，模型可能并未真正学到鲁棒性。
