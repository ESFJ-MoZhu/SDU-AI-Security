# SDU-AI-Security
计算机安全知识点梳理
粗略的看了一遍，有些地方可能有遗漏，还请见谅，欢迎一起补充~
## 已知的考试形式
类似机器学习(CYX)的形式，重点复习实验内容。
## 对抗样本
* 出现攻击与消失攻击
* 距离函数
* 定向攻击与非定向攻击
* 训练与攻击其实是对偶关系，训练是想让学习一个分类器，使得输入的预测结果与真实标签尽量接近，而攻击是找到一个扰动使损失函数最大化，从而让模型预测错误
* 梯度下降的思路，找到一个小扰动 𝛿让损失函数最大化，结合泰勒展开等等。
* 线性模型的对抗样本，在高维空间中，每一维的扰动积累起来，可能导致整体输出的大幅度变化，也会收到攻击
* DNN大多是分段线性的，比如卷积层，分段线性函数ReLU,Sigmoid函数不是线性的，但在一定范围内近似为线性。神经网络的线性特性导致了对抗样本的脆弱性
* 对抗样本的对抗扰动不是随机的，是需要计算梯度最大处刻意安排
* 不同对抗攻击方法原理类似，差异会因为`损失函数`,`约束条件`,`优化算法`而不同。
* 投影梯度下降法，Carlini-Wagner攻击，单像素攻击，Universal Adversarial Perturbation通用对抗扰动，Zeroth Order Optimization(ZOO)零阶优化方法
* Kerckhoffs原则:即便攻击者了解系统的设计和算法，只要密钥安全，系统就应该安全。
* 防御策略:1数据预处理，输入模型前移除对抗噪声。2模型加固，修改模型架构，训练流程以提升鲁棒性。(对抗训练，正则化，网络结构改进)3对抗样本检测，(输入特征分析，置信度阈值，特定网络检测器
* 防御性蒸馏
* 对抗训练，在训练过程中引入对抗扰动，强制模型在有攻击的情况下也能正确分类。限制:损失函数不是连续可微的，因为神经网络中的 ReLU 和 max-pooling 操作会引入非光滑性,我们只能近似找到最优扰动，而非严格意义上的全局最优.慢且不具可扩展性, 训练速度慢 2~20 倍,如果扰动弱，模型可能并未真正学到鲁棒性。
## 模型后门与数据投毒
### 数据投毒事后门攻击的主要手段
* Outsourcing Attack（外包攻击）,中间人搞事
* Pretrained Attack（预训练模型攻击），攻击者可以下载一个流行的预训练模型，在其中植入后门再重新发布给公众使用。开源社区中流行的预训练模型极易成为攻击的载体。思路:诱导模型增强局部链接，1.寻找最大化激活某个神经元的 pattern 2.逆向生成最大化某类别的训练数据 3.逆向数据 + pattern => 重新训练模型
* Data Collection Attack （数据收集攻击）受害者从公共来源收集数据，却没有意识到其中一些数据已经被投毒
* Targeted Clean-Label Attack 有针对性的干净标签攻击，攻击者不能直接篡改标签，只能投毒图像，感觉像对抗样本
* Watermarking(水印攻击),Multiple Instance Attack(多实例攻击),Collaborative Learning Attack(联邦学习攻击),Post-Deployment Attack（部署后攻击）方式可能是:攻击者入侵系统或服务器后，获得模型访问权限，直接在模型本身上动手脚，权重篡改攻击：修改模型权重，实现后门效果。比特翻转攻击：翻转存储模型的内存中的比特位，从而改变模型行为，Code Poisoning Attack(代码投毒攻击）代码中嵌入了后门逻辑，旨在污染训练后的模型
* 用于善意目的的后门攻击,水印，知识产权保护
### 防御手段
* ABS（Activation-based Backdoor Scanning, 基于激活的后门检测）方法。后门神经元在被特定触发器激活时，会导致目标输出类别异常激活。这种现象体现在某些神经元的激活值达到特殊水平时，网络输出几乎只依赖这些神经元，而对其他神经元不敏感。正常模型不会出现“单个神经元控制输出”的异常模式。
## 模型提取
方程求解方法与通过学习模拟的方法
攻击者（adversarial client）通过尽可能少的查询，学习到目标模型 𝑓的“近似拷贝” 𝑓′,使得 𝑓′(𝑥) 和f(x) 在 99.9% 以上输入上输出一致。
* 对逻辑回归模型的提取攻击，变换方程根据输入输出推测w和b
* Generic Equation-Solving Attack（通用方程求解攻击）攻击者准备一批输入 X,这些输入送入远程的机器学习服务,服务返回置信度,用优化算法（如梯度下降）反复调整 W，使损失最小。总结来说构造输入 → 查询模型 → 收集输出 → 拟合自己的神经网络参数
* 模型反演攻击（Model Inversion Attack），利用模型提取的结果，推出训练数据
* Extracting a Decision Tree输入𝑥和x′仅有一个特征不同。如果x和x′最终落在不同的叶节点，说明树在这个特征上进行了分裂。通过构造和查询不同输入，可以一步步推断出决策树在各特征上的分裂点，最终还原整棵树
* Generic Model Retraining Attack（通用模型重训练攻击）远不如方程求解法高效。主动学习可以大幅提升模型“扒取”效率，但在深度非线性模型下仍比直接方程求解低效许多
### 防御
* API最小化是一种防御策略,只返回类别标签，不返回置信度分数。但是，如果攻击者能用成员资格查询，依然能学习模型（如低效攻击）
* 限制预测信息，例如仅返回类别标签，而不返回或修改/隐藏/四舍五入置信度值
* 使用多个模型的组合来增加攻击难度，提高鲁棒性
* 通过差分隐私技术保护模型参数，防止攻击者通过查询推测模型内部信息。
### 学习模拟的思路
* Knockoff nets 拿别人的输出自己学
* 半监督学习
* 借助密码分析方法进行分析:ReLU的二阶导为0 & 有限差分
*  Data-Free Model Extraction ......
 ## 数据提取 
 * 模型反演攻击（Model Inversion Attack），利用模型提取的结果，推出训练数据
 * Data Extraction Attack（数据提取攻击）
 * Data Stealing Attack（数据窃取攻击）
 * Training Data Extraction Attack（训练数据提取攻击）
 * Model Memorization Attack（模型记忆攻击）
 * Model Inversion Attack（模型反演攻击）
```
   模型容量 (Model Capacity)
定义：DNN的参数数量通常远大于训练样本数，使其具有记住训练数据的能力。

示例：ResNet-50有约2500万个参数，而CIFAR-10数据集仅包含6万张图像。

优化目标 (Optimization Objective)
定义：DNN的训练目标是最小化损失函数（如交叉熵），可能导致对噪声或异常样本的过拟合。

示例：若某张图像被错误标注，DNN可能通过记住该样本来降低损失。

数据分布 (Data Distribution)
定义：训练数据中的噪声、重复样本或敏感信息可能被DNN记住。

示例：医疗数据中的罕见病例可能被DNN记住，导致隐私泄露。
```
### 方法
* 梯度下降
```
函数 MI-FACE(label, α, β, γ, λ):
    c(x) 定义为 1 - f_label(x) + AUXTERM(x)  # 损失函数定义
    x₀ ← 0                                   # 初始化输入

    对 i = 1 到 α:
        xᵢ ← PROCESS(xᵢ₋₁ - λ ⋅ ∇c(xᵢ₋₁))    # 通过梯度下降更新输入

        如果 c(xᵢ) ≥ max(c(xᵢ₋₁), ..., c(xᵢ₋β)) :
            break                             # 若损失大于过去β次中的最大值，则终止

        如果 c(xᵢ) ≤ γ :
            break                             # 如果损失小于阈值γ，则终止

    返回 [argminₓ c(xᵢ), minₓ c(xᵢ)]         # 返回损失最小的输入和对应的损失值
```
### 防御
* 四舍五入处理，通过减少置信度值的精度，来削弱攻击者从模型输出中获取过多信息的能力
* Exposure-based Testing Method ，通过加入随机的训练数据(金丝雀canary)，看看模型查询模型，看其是否能够回忆出。若能回忆出，说明模型具有较强的记忆能力，存在泄漏训练数据的风险

总结

```
无意记忆的发生并不依赖于过拟合的产生，甚至都不是过度训练导致的
不常见的随机训练数据会在模型达到最大效用之前就被记住了
weight decay, dropout, and quantization 三种 regularization 方法均不能解决记忆问题
使用 Differential Privacy 可以防止模型记忆化，但模型能力也被削弱
```
## 隐私推理
*  Membership Inference Attack (MIA)判断一个用户是否参与了模型的训练数据。最常见的思路从数据集 𝐷中抽样出多个子集,在每个子集上训练一个模型,选择其中一个模型作为目标模型, 其余的模型作为影子模型
*  MIA局限性:需要构造影子模型（shadow models），增加攻击复杂度和成本.假设攻击者能够获取某些数据或具有先验知识。模型必须发生过拟合，攻击才能有效。无过拟合时，成员和非成员样本表现趋同，难以区分.当前大多数方法仅适用于分类模型。攻击在大模型上效果下降，主要在小模型（如简单CNN）上实验验证
*  基于指标的异常检测:预测正确性(预测正确的就是成员),预测损失(低于训练样本平均损失的是成员),预测置信度(有概率接近1的是成员),预测熵(低概率熵的是成员),修正预测熵
*  Property Inference属性推断.举例：即使性别不是数据集的特征之一，也可以从训练模型的数据集中推断出女性和男性患者的比例
*  Attribute Inference特征值推断
### 防范措施
对数据隐私的保护
* 推理阶段的数据隐私保护算法：在模型的推理阶段施加防御手段；
* 训练阶段的数据隐私保护算法：在模型的训练阶段施加防御手段；
对模型版权的保护
* 数字水印（Digital Watermark）
* 用 AE 的思想抵抗成员推理模型的分类结果。对模型的原始输出 𝑠（即每个类别的预测概率）添加一个小的扰动 r，得到新的输出 𝑠+𝑟。希望添加的扰动𝑟要尽量小，这样不会影响模型的原有预测性能。扰动后的输出 𝑠+𝑟要让攻击者的推理模型 𝑔输出“成员概率”为 0.5(最好的情况)，也就是“非常不确定”，无法判断是不是成员
* 白盒水印:嵌入水印，提取水印，验证所有权。添加惩罚项将水印嵌入模型，提取实用对应的提取矩阵提取。
* 黑盒水印:水印的验证不需要模型参数,是一种后门攻击。嵌入方式:训练时加入特定“水印”样本到训练数据中。验证方式:在不接触模型内部参数的情况下，只用这些特殊水印样本作为输入，观察模型是否给出特定输出。如果模型能识别这些水印数据，则说明该模型嵌入了水印
## 差分隐私
对于只相差一条记录（行）的两个数据集（D 和 D'），无论用哪个数据集来计算，最终输出的结果（Outcome）都应该“非常相似”，这样就能保证，单个人的数据是否在数据集里，不会对分析结果产生显著影响，从而保护了个人隐私。
* 𝜀-差分隐私
* 随机响应 随机响应引入了“故意的随机噪声”，它的目的是牺牲一定的准确性，来换取更强的隐私保护。单个个体的回答：不准确，因为有概率是故意“反着回答”的。整体统计结果：准确！虽然每个人的回答可能掺杂了随机性，但通过大量样本统计，可以用数学方法校正偏差，还原总体的真实分布
......
## 能量延迟
* 影响每次推理所消耗能量的主要因素：输入数据在计算过程中所需的算术运算次数，访问内存的次数
* 海绵样本:攻击者精心设计某些输入，这些输入会让模型在推理时陷入大量计算和内存访问，大幅度提高耗时和能耗
## 可解释性
Fairness（公平性）Privacy（隐私性）Safety and Robustness（安全和鲁棒性）Causality（因果性）Trust（可信赖）
并没有一个很严格的定义
* 机器学习的步骤：1准备输入数据，原始数据可能分布不均，经过标准化处理（如零均值归一化）后，数据分布变得更规范、便于模型学习。标准化、归一化等预处理步骤是机器学习第一步，有助于模型更快更好地收敛2选择优化器，优化器的选择会影响模型训练的速度和最终效果，目的是在损失面上高效地找到最低点3定义损失函数损失函数的设计和正则化方法直接影响模型泛化能力和决策边界的平滑性
* 模型无关（Model-agnostic）方法，指的是不依赖于特定模型结构或类型，可以应用于任何机器学习模型的解释或分析方法。换句话说，无论你的模型是决策树、神经网络、支持向量机还是别的，只要能输入和输出，都可以用模型无关方法来解释和分析
* 后验可解释性：1.局部解释 Feature Importances（特征重要性）说明每个特征对当前决策的贡献有多大。Rule Based（基于规则的）用规则形式（如 if-then 语句）描述模型在某一具体决策时的逻辑。Saliency Maps（显著图）常用于图像任务，突出显示输入中对预测最重要的区域。Prototypes/Example Based（基于原型/样本的）通过与典型样本（prototypes）对比，解释当前样本为什么被分类为某一类。Counterfactuals（反事实解释）指明如果输入稍作改变（如某特征换成别的值），预测会不会不同。  2.全局解释 Collection of Local Explanations（局部解释的集合）通过汇总大量单个样本的解释，得到模型整体的解释。Representation Based（基于表示的）分析模型中间层、嵌入空间等整体性表示来解释模型。Model Distillation（模型蒸馏）用一个简单、易解释的模型去拟合复杂模型的决策，从而间接解释复杂模型。Summaries of Counterfactuals（反事实总结）总结不同输入下反事实分析结果，解释模型对变化的整体敏感性。
评估方法可以从这两个维度，即 模型专用 vs. 模型无关  与  全局解释 vs. 局部解释
* LIME  模型无关的，局部的
* LEMNA
* SHAP 模型无关的，全局的
* 提取决策树，读取决策树的分裂规则，得到整体决策依据。这个方法是模型专用的（不是模型无关），全局解释
**Q**特征是像素，扰动很小”这个特殊场景下，如何高效地评估像素对模型输出的影响
```
方法
1.对于每个像素，分别把它加/减一点点，每次都做一次前向传播（forward pass）计算输出，再看输出变化。 缺点：如果图片有成千上万个像素，计算量巨大
2.只需做一次反向传播（backward pass），就能得到所有像素对输出分数的“微小贡献”（即梯度），利用反向传播的高效性
```
* Integrated Gradients（积分梯度）通过从某个基线输入（如全黑图像、均值图像等）逐步平滑地变成真实输入，在这条路径上累积梯度，这样既能反映出重要特征的真实贡献，又能避免传统梯度解释在某些区域不敏感的问题。
* CAM 全局平均池化（GAP）层输出的特征与最后一层卷积特征的权重结合，生成每个类别的激活热力图，可视化模型关注区域。只能用于最后一层特征图和全连接层之间有GAP操作的网络结构，不适合任意结构的深度网络，容易饱和（即所有区域都亮或者都暗），生成“没用的图”。比较简单，易于实现
* Grad-CAM Grad-CAM 能将CAM思想推广到任意非线性结构，适用于更广泛的卷积神经网络。对目标类别分数关于特征图做梯度，作为权重加权特征图，实现对关注区域的可视化。适用于任意CNN结构。`步骤`1.求目标类别的分数y^c对特征图A的梯度，得到梯度矩阵2. 全局平均梯度：对每个特征图的梯度在空间维度求平均3.加权特征图：将权重与特征图相乘并求和，得到粗热力图
* Prototype-based 基于原型的方法，影响函数（Influence function），目标是识别最能影响某个预测结果的训练样本
* 定义`缺失特征`,有时候也会有缺点，比如遇到黑色汽车，灰色的物品什么的，巴拉巴拉。可以通过`有意义的扰动`解决,把关键点抹掉
  *   Integrated gradients：把0（黑色像素）作为“缺失特征”。
  *   Zintgraf et al：用填补/修补（inpainting）的方法来模拟缺失特征。
  *   LIME：用均值（灰色像素）表示缺失特征。
  *   Shapley：同样用均值（灰色像素）作为缺失特征。
### 公平性
* 群体公平,可能导致模型可用性很差
* 准确率公平，每个群体被准确预测（预测值等于真实值）的概率是一样的。虽然整体准确率一样，但“错误类型”不同（假阳性/假阴性）仍会造成实际不公平。
* Equal Opportunity（机会均等）TPP
* FPP
* TPP + FPP = Equalized Odds（机会均等）
## 应用层安全
### 移动端智能模型的要求
```
要求低延时
  自动驾驶
  扫地机器人导航避障
要求离线使用
  偏远地区电力巡检
  对云端服务器负载过大
  支付宝将扫五福功能由云端移到终端执行[1]
要求保护隐私
  GDPR要求个人生物识别信息存储在本地，并在本地处理[2]
  人脸识别，指纹识别等
```
### 移动端模型特点
* 模型结构较简单,以CNN居多,大部分是32位浮点数计算
* 运行开销小
### 移动端模型优势
* 延迟：不需要通过网络连接发送请求并等待响应。
* 可用性：即使在网络覆盖范围之外，应用也能运行。
* 速度：专用于神经网络处理的新硬件提供的计算速度明显快于单纯的通用CPU。
* 隐私：数据不会离开Android设备。
* 费用：所有计算都在Android设备上执行，不需要额外的云服务器。
* 个性化（可扩展性）：可以为用户定制机器学习服务。
### 移动端模型优化
* 权重量化:指用低位宽表示类型为32位浮点同构参数。网络参数包括权重、激活值、梯度和误差等量，可使用统一的位宽（如16-bit、8-bit、2-bit和1-bit等）。
* 权重稀疏化:通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。
* 通道剪枝:在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度。
* 网络蒸馏:通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练。
### 模型参数安全
* 明文模型：加载判断，十六进制判断
* 保护措施：混淆，调用代码混淆，模型调用功能在native层实现，完整性检验(无法阻止模型窃取)，动态下载(api接口保护)，自定义OP，水印，闭源框架，加密(无法对抗内存dump)
* 张量 Tensor，对张量的不当处理可能引发问题
* 底层软件的安全，相应库的漏洞
......
## 供应链安全
指AI模型及其相关依赖（数据、代码、模型、库等）在开发、传输、部署等全流程中可能遭受的攻击与威胁。
(偷懒)
